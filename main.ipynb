{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Natural Language Processing with Disaster Tweets Competition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Depedencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing, svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data//train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       " 1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       " 2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       " 3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       " 4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       " id             0\n",
       " keyword       61\n",
       " location    2533\n",
       " text           0\n",
       " target         0\n",
       " dtype: int64,\n",
       " array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "        'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "        'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "        'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "        'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "        'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "        'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "        'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "        'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "        'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "        'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n",
       "        'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n",
       "        'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n",
       "        'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n",
       "        'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n",
       "        'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n",
       "        'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n",
       "        'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n",
       "        'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n",
       "        'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n",
       "        'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n",
       "        'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
       "        'fire', 'fire%20truck', 'first%20responders', 'flames',\n",
       "        'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n",
       "        'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n",
       "        'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n",
       "        'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n",
       "        'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n",
       "        'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n",
       "        'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n",
       "        'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n",
       "        'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n",
       "        'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n",
       "        'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n",
       "        'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n",
       "        'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n",
       "        'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n",
       "        'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n",
       "        'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n",
       "        'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n",
       "        'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n",
       "        'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n",
       "        'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n",
       "        'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n",
       "        'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n",
       "        'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(), train_df.isna().sum(), train_df['keyword'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Label = 0 -> Non Disaster Tweet</li>\n",
    "<li>Label = 1 -> Disaster Tweet</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"What's up man?\", 'I love fruits', 'Summer is lovely',\n",
       "       'My car is so fast', 'What a goooooooaaaaaal!!!!!!'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Non Disaster Tweet\n",
    "train_df[train_df[\"target\"] == 0][\"text\"].values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       "       'Forest fire near La Ronge Sask. Canada',\n",
       "       \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
       "       '13,000 people receive #wildfires evacuation orders in California ',\n",
       "       'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Disaster Tweet\n",
    "train_df[train_df[\"target\"] == 1][\"text\"].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A way of verifying if a tweet is related to a disaster or not, may be by looking at the words contained in the tweet<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's put into vectors the words and their count from each of the tweets<p>\n",
    "<p>We will use CountVectorizer from sckikit-learn<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '13', 'alaska', 'all', 'allah', 'are', 'as', 'asked',\n",
       "       'being', 'by', 'california', 'canada', 'deeds', 'earthquake',\n",
       "       'evacuation', 'expected', 'fire', 'forest', 'forgive', 'from',\n",
       "       'got', 'in', 'into', 'just', 'la', 'may', 'near', 'no', 'notified',\n",
       "       'of', 'officers', 'or', 'orders', 'other', 'our', 'people',\n",
       "       'photo', 'place', 'pours', 'reason', 'receive', 'residents',\n",
       "       'ronge', 'ruby', 'sask', 'school', 'sent', 'shelter', 'smoke',\n",
       "       'the', 'this', 'to', 'us', 'wildfires'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the unique words in teh first 5 tweets\n",
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 54)\n",
      "[[0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We are checking the occurences of the different words that exist in all 5 first tweets, in the first tweet.\n",
    "<p>The 0 values mean that the correspondent word is not in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the tweets\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "# For the test we only use transform in order to only use the set of the train words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., 'ûónegligence', 'ûótech', 'ûówe'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It is assumed that the presence of a word or set of words gives us info into if the tweet is disaster related or not. This relationship is linear, so let's build a linear model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.RidgeClassifier(alpha=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61894025, 0.59903897, 0.67908903])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeClassifier(alpha=30)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(alpha=30)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeClassifier(alpha=30)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           2\n",
       "2           3\n",
       "3           9\n",
       "4          11\n",
       "        ...  \n",
       "3258    10861\n",
       "3259    10865\n",
       "3260    10868\n",
       "3261    10874\n",
       "3262    10875\n",
       "Name: id, Length: 3263, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 21637)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [test_df['id'], preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame({'id': test_df['id'], 'target':preds})\n",
    "predict.to_csv(\"./predictions/predictions_4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
